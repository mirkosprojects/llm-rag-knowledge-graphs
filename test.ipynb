{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama and Huggingface packages\n",
    "%pip install llama-index-llms-ollama llama-index-embeddings-huggingface llama-index networkx llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b05d81",
   "metadata": {},
   "source": [
    "### RAG using Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba254a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The teaching faculty at a specific institution in Romania, which I'll refer to as a key location, includes an individual who instructs in semantic web. This instructor has expertise in databases and conception modelling, suggesting a broad range of academic subjects. Teaching in this field can be demanding due to the need for patience, long hours spent with students, and navigating various topics related to data management and system design.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files/test').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Build the index with local embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Who teaches in romania and what courses do they teach?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34eb7a3",
   "metadata": {},
   "source": [
    "### RAG using KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d485a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna is located in Cluj Napoca, which is a city in Romania. She teaches certain subjects that are relevant to her academic background, including semantic web, databases, and conception modelling.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files/test').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    # max_triplets_per_chunk=2,\n",
    "    storage_context=storage_context,\n",
    "    embed_model='local',\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = kg_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Who teaches in romania and what courses do they teach?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1378b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romanian professors typically teach courses in Romanian language and literature, history, philosophy, and cultural studies. They may also instruct in fields such as economics, politics, or international relations, reflecting the country's rich academic heritage.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files/test').load_data()\n",
    "\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build Knowledge Graph Index\n",
    "graph_store = SimpleGraphStore()\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    graph_store=graph_store,\n",
    "    embed_model='local',\n",
    "    llm=llm,\n",
    "    max_triplets_per_chunk=10\n",
    ")\n",
    "\n",
    "# Create query engines\n",
    "kg_engine = kg_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Combine into a Router engine\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=kg_engine,\n",
    "        description=\"Use for fact-based or entity relationship queries.\"\n",
    "    )\n",
    "]\n",
    "router_engine = RouterQueryEngine.from_defaults(query_engine_tools=tools, llm=llm)\n",
    "\n",
    "# Query\n",
    "response = router_engine.query(\"What are the courses taught by a romanian professor?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c9596",
   "metadata": {},
   "source": [
    "VERSION BOTH VECTOR AND KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d671fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f56fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "A collaborative research project has been initiated by two prominent researchers, Dr. Grant and Dr. Reed. Dr. Lisa Grant is a molecular biologist, while Dr. Alan Reed is a data scientist from NovaTech, an AI company based in the U.S. The nature of their collaboration involves data analysis.\n",
      "\n",
      "Document 2:\n",
      "Dr. Lisa Grant and Dr. Alan Reed have partnered to work on a research project under BioCore's Emerging Innovations Division. Their collaboration combines expertise from molecular biology and data science, reflecting the interdisciplinary approach often seen in AI-driven research projects.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files/test').load_data()\n",
    "\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build Knowledge Graph Index\n",
    "graph_store = SimpleGraphStore()\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    graph_store=graph_store,\n",
    "    embed_model='local',\n",
    "    llm=llm,\n",
    "    max_triplets_per_chunk=10\n",
    ")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Create query engines\n",
    "kg_engine = kg_index.as_query_engine(llm=llm)\n",
    "vector_engine = vector_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Combine into a Router engine\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=kg_engine,\n",
    "        description=\"Use for fact-based or entity relationship queries.\"\n",
    "    ),\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_engine,\n",
    "        description=\"Use for general or contextual document questions.\"\n",
    "    )\n",
    "]\n",
    "router_engine = RouterQueryEngine.from_defaults(query_engine_tools=tools, llm=llm)\n",
    "\n",
    "# Query\n",
    "response = router_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb1cb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisa Grant is an individual with expertise in firefighting. She is currently collaborating with Dr. Alan Reed, a data scientist from NovaTech, on a data analysis project.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Who is Lisa Grant and what is she collaborating on?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e2b982",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed1eeb08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:59\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     58\u001b[39m     embed_model = OpenAIEmbedding()\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/embeddings/openai/utils.py:103\u001b[39m, in \u001b[36mvalidate_openai_api_key\u001b[39m\u001b[34m(api_key)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[31mValueError\u001b[39m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m documents = SimpleDirectoryReader(\u001b[33m\"\u001b[39m\u001b[33mfiles/test\u001b[39m\u001b[33m\"\u001b[39m).load_data()\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Step 3: Create Knowledge Graph Index with the custom LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m index = \u001b[43mKnowledgeGraphIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# embed_model='local',\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_triplets_per_chunk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass the Ollama model to the index\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Step 4: Query\u001b[39;00m\n\u001b[32m     20\u001b[39m query_engine = index.as_query_engine()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/core/indices/base.py:122\u001b[39m, in \u001b[36mBaseIndex.from_documents\u001b[39m\u001b[34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n\u001b[32m    115\u001b[39m nodes = run_transformations(\n\u001b[32m    116\u001b[39m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    117\u001b[39m     transformations,\n\u001b[32m    118\u001b[39m     show_progress=show_progress,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/core/indices/knowledge_graph/base.py:99\u001b[39m, in \u001b[36mKnowledgeGraphIndex.__init__\u001b[39m\u001b[34m(self, nodes, objects, index_struct, llm, embed_model, storage_context, kg_triplet_extract_template, max_triplets_per_chunk, include_embeddings, show_progress, max_object_length, kg_triplet_extract_fn, **kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m._kg_triplet_extract_fn = kg_triplet_extract_fn\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m._llm = llm \u001b[38;5;129;01mor\u001b[39;00m Settings.llm\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = embed_model \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_model\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    102\u001b[39m     nodes=nodes,\n\u001b[32m    103\u001b[39m     index_struct=index_struct,\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m     **kwargs,\n\u001b[32m    108\u001b[39m )\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# TODO: legacy conversion - remove in next release\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/core/settings.py:64\u001b[39m, in \u001b[36m_Settings.embed_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model = \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._embed_model.callback_manager = \u001b[38;5;28mself\u001b[39m._callback_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:66\u001b[39m, in \u001b[36mresolve_embed_model\u001b[39m\u001b[34m(embed_model, callback_manager)\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     62\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     63\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m         )\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     67\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not load OpenAI embedding model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConsider using embed_model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mVisit our documentation for more embedding options: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33membeddings.html#modules\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m         )\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model.startswith(\u001b[33m\"\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
     ]
    }
   ],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Step 1: Use Ollama as the LLM\n",
    "llm = Ollama(model=\"llama3.2:latest\")  # or \"mistral\", \"phi3\", etc.\n",
    "\n",
    "# Step 2: Load documents\n",
    "documents = SimpleDirectoryReader(\"files/test\").load_data()\n",
    "\n",
    "# Step 3: Create Knowledge Graph Index with the custom LLM\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    # embed_model='local',\n",
    "    max_triplets_per_chunk=2,\n",
    "    use_async=True,\n",
    "    llm=llm  # pass the Ollama model to the index\n",
    ")\n",
    "\n",
    "# Step 4: Query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Who is Lisa Grant and what is she working on?\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7835665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, there are no documents provided for summarization.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files/test').load_data()\n",
    "\n",
    "# Set up LLM and embeddings\n",
    "llm = Ollama(model=\"llama3.2:latest\", timeout=500)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Apply settings (optional global config)\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm\n",
    "\n",
    "# Create the Knowledge Graph Index\n",
    "index = KnowledgeGraphIndex.from_documents(documents)\n",
    "\n",
    "# Set up query engine (uses semantic similarity over graph + LLM reasoning)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the knowledge graph\n",
    "response = query_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

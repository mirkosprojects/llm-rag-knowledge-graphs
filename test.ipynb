{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama and Huggingface packages\n",
    "%pip install llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f31f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba254a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document summaries:\n",
      "\n",
      "One text describes a common human-companion relationship, highlighting the unique qualities and roles that dogs play in people's lives. It showcases their versatility as pets and the positive impact they have on owners.\n",
      "\n",
      "The other text delves into ancient Rome's history and achievements, featuring its architectural marvels, engineering feats, and cultural legacies. It highlights the Roman Empire's vast territories, military prowess, and enduring influence on modern societies.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

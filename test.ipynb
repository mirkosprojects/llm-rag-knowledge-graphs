{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama and Huggingface packages\n",
    "%pip install llama-index-llms-ollama llama-index-embeddings-huggingface llama-index networkx llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f31f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba254a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Dogs are considered loyal companions due to their unique traits and personalities. They provide comfort, protection, and love to their owners, making them beloved pets worldwide. Whether active or relaxed, dogs bring joy to people's lives.\n",
      "\n",
      "Document 2:\n",
      "The city of Rome was the heart of the Roman Empire in ancient times. The Romans were skilled engineers who built grand structures like the Colosseum and Pantheon. They created extensive networks of roads, aqueducts, and bridges that connected their territories and left a lasting impact on modern societies.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d485a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document summaries cannot be provided as there are no documents to summarize.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "\n",
    "# Build the index with local embeddings\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    graph_store=graph_store,\n",
    "    embed_model='local',\n",
    "    llm=llm,\n",
    "    max_triplets_per_chunk=10\n",
    ")\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = kg_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION HECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Dogs are widely regarded as loyal companions due to their unique traits and abilities. They provide comfort, protection, and unwavering love to their owners, making them beloved pets worldwide.\n",
      "\n",
      "Document 2:\n",
      "The Roman Empire's heart was the city of Rome itself, renowned for its grand architecture, engineering prowess, and extensive territorial network. The Roman Republic fostered a skilled society that left lasting impacts on art, law, and governance in modern societies today.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build Knowledge Graph Index\n",
    "graph_store = SimpleGraphStore()\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    graph_store=graph_store,\n",
    "    embed_model='local',\n",
    "    llm=llm,\n",
    "    max_triplets_per_chunk=10\n",
    ")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Create query engines\n",
    "kg_engine = kg_index.as_query_engine(llm=llm)\n",
    "vector_engine = vector_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Combine into a Router engine\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=kg_engine,\n",
    "        description=\"Use for fact-based or entity relationship queries.\"\n",
    "    ),\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_engine,\n",
    "        description=\"Use for general or contextual document questions.\"\n",
    "    )\n",
    "]\n",
    "router_engine = RouterQueryEngine.from_defaults(query_engine_tools=tools, llm=llm)\n",
    "\n",
    "# Query\n",
    "response = router_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

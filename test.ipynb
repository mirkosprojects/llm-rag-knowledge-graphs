{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama and Huggingface packages\n",
    "%pip install llama-index-llms-ollama llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f31f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba254a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intelSys/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 describes a dog as a loyal companion with unique traits and personalities. Dogs provide comfort, protection, and love to their owners, making them beloved pets worldwide.\n",
      "\n",
      "Document 2 delves into ancient Roman history, describing the city of Rome as the heart of the vast empire. The Romans were skilled engineers who built iconic structures and an extensive network of roads and aqueducts that connected their territories, ultimately establishing a legacy in art, law, and governance.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"Summarize each document in a few sentences.\")\n",
    "print(response)\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERSION HECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"llama3.2:latest\", request_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.indices.knowledge_graph import KnowledgeGraphIndex\n",
    "from llama_index.query_engine import RouterQueryEngine\n",
    "from llama_index.tools import QueryEngineTool\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader('files').load_data()\n",
    "\n",
    "\n",
    "# Set up local embeddings (you can specify a different model too)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build Knowledge Graph Index\n",
    "graph_store = SimpleGraphStore()\n",
    "kg_index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    graph_store=graph_store,\n",
    "    llm=llm,\n",
    "    max_triplets_per_chunk=10\n",
    ")\n",
    "\n",
    "# Build the index with local embeddings\n",
    "vector_index = VectorStoreIndex.from_documents(documents, llm=llm)\n",
    "\n",
    "# Create query engines\n",
    "kg_engine = kg_index.as_query_engine(llm=llm)\n",
    "vector_engine = vector_index.as_query_engine(llm=llm)\n",
    "\n",
    "# Combine into a Router engine\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=kg_engine,\n",
    "        description=\"Use for fact-based or entity relationship queries.\"\n",
    "    ),\n",
    "    QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_engine,\n",
    "        description=\"Use for general or contextual document questions.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "router_engine = RouterQueryEngine.from_defaults(tools=tools, llm=llm)\n",
    "\n",
    "# Set up query engine using Ollama\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query\n",
    "response = query_engine.query(\"What are the main entities and what do they do?\")\n",
    "print(response)\n"
=======
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI models for embeddings\n",
    "# from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# documents = SimpleDirectoryReader('files').load_data()\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"summarize each document in a few sentences\")\n",
    "\n",
    "# print(response)\n"
>>>>>>> 951e97d39c181b8d2325bc50a334f5409acc963f
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelSys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
